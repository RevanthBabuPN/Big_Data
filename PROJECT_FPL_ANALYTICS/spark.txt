20/12/01 12:31:19 WARN Utils: Your hostname, sreyans-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)
20/12/01 12:31:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
20/12/01 12:31:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
20/12/01 12:31:29 INFO SparkContext: Running Spark version 3.0.1
20/12/01 12:31:29 INFO ResourceUtils: ==============================================================
20/12/01 12:31:30 INFO ResourceUtils: Resources for spark.driver:

20/12/01 12:31:30 INFO ResourceUtils: ==============================================================
20/12/01 12:31:30 INFO SparkContext: Submitted application: FPL_analytics
20/12/01 12:31:30 INFO SecurityManager: Changing view acls to: sreyans
20/12/01 12:31:30 INFO SecurityManager: Changing modify acls to: sreyans
20/12/01 12:31:30 INFO SecurityManager: Changing view acls groups to: 
20/12/01 12:31:30 INFO SecurityManager: Changing modify acls groups to: 
20/12/01 12:31:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sreyans); groups with view permissions: Set(); users  with modify permissions: Set(sreyans); groups with modify permissions: Set()
20/12/01 12:31:31 INFO Utils: Successfully started service 'sparkDriver' on port 46741.
20/12/01 12:31:32 INFO SparkEnv: Registering MapOutputTracker
20/12/01 12:31:32 INFO SparkEnv: Registering BlockManagerMaster
20/12/01 12:31:32 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/12/01 12:31:32 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/12/01 12:31:32 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/12/01 12:31:32 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5e063467-4451-4e79-b51f-5123116347fb
20/12/01 12:31:32 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
20/12/01 12:31:32 INFO SparkEnv: Registering OutputCommitCoordinator
20/12/01 12:31:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/12/01 12:31:33 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.2.15:4040
20/12/01 12:31:34 INFO Executor: Starting executor ID driver on host 10.0.2.15
20/12/01 12:31:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43733.
20/12/01 12:31:35 INFO NettyBlockTransferService: Server created on 10.0.2.15:43733
20/12/01 12:31:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/12/01 12:31:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.2.15, 43733, None)
20/12/01 12:31:35 INFO BlockManagerMasterEndpoint: Registering block manager 10.0.2.15:43733 with 413.9 MiB RAM, BlockManagerId(driver, 10.0.2.15, 43733, None)
20/12/01 12:31:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.2.15, 43733, None)
20/12/01 12:31:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.2.15, 43733, None)
20/12/01 12:31:36 INFO SingleEventLogFileWriter: Logging events to file:/tmp/spark/local-1606806094213.inprogress
20/12/01 12:31:39 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/local/spark-3.0.1-bin-hadoop3.2/spark-warehouse').
20/12/01 12:31:39 INFO SharedState: Warehouse path is 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/spark-warehouse'.
20/12/01 12:31:42 INFO InMemoryFileIndex: It took 173 ms to list leaf files for 1 paths.
20/12/01 12:31:43 INFO InMemoryFileIndex: It took 22 ms to list leaf files for 1 paths.
20/12/01 12:31:51 INFO FileSourceStrategy: Pruning directories with: 
20/12/01 12:31:51 INFO FileSourceStrategy: Pushed Filters: 
20/12/01 12:31:51 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
20/12/01 12:31:51 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/12/01 12:31:53 INFO AsyncEventQueue: Process of event SparkListenerSQLExecutionStart(0,csv at NativeMethodAccessorImpl.java:0,org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:723)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:282)
py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
py4j.commands.CallCommand.execute(CallCommand.java:79)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748),== Parsed Logical Plan ==
GlobalLimit 1
+- LocalLimit 1
   +- Filter (length(trim(value#6, None)) > 0)
      +- Project [value#0 AS value#6]
         +- Project [value#0]
            +- Relation[value#0] text

== Analyzed Logical Plan ==
value: string
GlobalLimit 1
+- LocalLimit 1
   +- Filter (length(trim(value#6, None)) > 0)
      +- Project [value#0 AS value#6]
         +- Project [value#0]
            +- Relation[value#0] text

== Optimized Logical Plan ==
GlobalLimit 1
+- LocalLimit 1
   +- Filter (length(trim(value#0, None)) > 0)
      +- Relation[value#0] text

== Physical Plan ==
CollectLimit 1
+- *(1) Filter (length(trim(value#0, None)) > 0)
   +- FileScan text [value#0] Batched: false, DataFilters: [(length(trim(value#0, None)) > 0)], Format: Text, Location: InMemoryFileIndex[file:/home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/players.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<value:string>
,org.apache.spark.sql.execution.SparkPlanInfo@ce798ca2,1606806111995) by listener EventLoggingListener took 1.686583875s.
20/12/01 12:31:54 INFO CodeGenerator: Code generated in 965.435716 ms
20/12/01 12:31:55 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 303.2 KiB, free 413.6 MiB)
20/12/01 12:31:55 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 413.6 MiB)
20/12/01 12:31:55 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.2.15:43733 (size: 27.4 KiB, free: 413.9 MiB)
20/12/01 12:31:55 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
20/12/01 12:31:55 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4238336 bytes, open cost is considered as scanning 4194304 bytes.
20/12/01 12:31:56 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
20/12/01 12:31:56 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/01 12:31:56 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
20/12/01 12:31:56 INFO DAGScheduler: Parents of final stage: List()
20/12/01 12:31:56 INFO DAGScheduler: Missing parents: List()
20/12/01 12:31:56 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/01 12:31:56 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 413.6 MiB)
20/12/01 12:31:56 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 413.6 MiB)
20/12/01 12:31:56 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.2.15:43733 (size: 5.3 KiB, free: 413.9 MiB)
20/12/01 12:31:56 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1223
20/12/01 12:31:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/01 12:31:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/12/01 12:31:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 7778 bytes)
20/12/01 12:31:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/12/01 12:31:58 INFO FileScanRDD: Reading File path: file:///home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/players.csv, range: 0-44032, partition values: [empty row]
20/12/01 12:31:58 INFO CodeGenerator: Code generated in 85.147509 ms
20/12/01 12:31:59 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1659 bytes result sent to driver
20/12/01 12:31:59 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2268 ms on 10.0.2.15 (executor driver) (1/1)
20/12/01 12:31:59 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/12/01 12:31:59 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.961 s
20/12/01 12:31:59 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/01 12:31:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
20/12/01 12:31:59 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 3.292045 s
20/12/01 12:31:59 INFO CodeGenerator: Code generated in 68.463731 ms
20/12/01 12:32:00 INFO FileSourceStrategy: Pruning directories with: 
20/12/01 12:32:00 INFO FileSourceStrategy: Pushed Filters: 
20/12/01 12:32:00 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/01 12:32:00 INFO FileSourceStrategy: Output Data Schema: struct<value: string>
20/12/01 12:32:00 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 303.2 KiB, free 413.3 MiB)
20/12/01 12:32:00 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 413.3 MiB)
20/12/01 12:32:00 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.2.15:43733 (size: 27.4 KiB, free: 413.9 MiB)
20/12/01 12:32:00 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
20/12/01 12:32:00 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4238336 bytes, open cost is considered as scanning 4194304 bytes.
20/12/01 12:32:00 INFO FileSourceStrategy: Pruning directories with: 
20/12/01 12:32:00 INFO FileSourceStrategy: Pushed Filters: 
20/12/01 12:32:00 INFO FileSourceStrategy: Post-Scan Filters: 
20/12/01 12:32:00 INFO FileSourceStrategy: Output Data Schema: struct<name: string, Id: string>
20/12/01 12:32:01 INFO CodeGenerator: Code generated in 68.524721 ms
20/12/01 12:32:01 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 303.2 KiB, free 413.0 MiB)
20/12/01 12:32:01 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 27.4 KiB, free 412.9 MiB)
20/12/01 12:32:01 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.2.15:43733 (size: 27.4 KiB, free: 413.8 MiB)
20/12/01 12:32:01 INFO SparkContext: Created broadcast 3 from javaToPython at NativeMethodAccessorImpl.java:0
20/12/01 12:32:01 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4238336 bytes, open cost is considered as scanning 4194304 bytes.
20/12/01 12:32:01 INFO SparkContext: Starting job: collectAsMap at /home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/mainspark.py:257
20/12/01 12:32:01 INFO DAGScheduler: Got job 1 (collectAsMap at /home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/mainspark.py:257) with 1 output partitions
20/12/01 12:32:01 INFO DAGScheduler: Final stage: ResultStage 1 (collectAsMap at /home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/mainspark.py:257)
20/12/01 12:32:01 INFO DAGScheduler: Parents of final stage: List()
20/12/01 12:32:01 INFO DAGScheduler: Missing parents: List()
20/12/01 12:32:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0), which has no missing parents
20/12/01 12:32:01 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.0 KiB, free 412.9 MiB)
20/12/01 12:32:01 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.0 KiB, free 412.9 MiB)
20/12/01 12:32:01 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.2.15:43733 (size: 7.0 KiB, free: 413.8 MiB)
20/12/01 12:32:01 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1223
20/12/01 12:32:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[15] at javaToPython at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
20/12/01 12:32:01 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/12/01 12:32:01 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 7778 bytes)
20/12/01 12:32:01 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/12/01 12:32:01 INFO FileScanRDD: Reading File path: file:///home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/players.csv, range: 0-44032, partition values: [empty row]
20/12/01 12:32:02 INFO CodeGenerator: Code generated in 77.358652 ms
20/12/01 12:32:02 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 37045 bytes result sent to driver
20/12/01 12:32:02 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 943 ms on 10.0.2.15 (executor driver) (1/1)
20/12/01 12:32:02 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/12/01 12:32:02 INFO DAGScheduler: ResultStage 1 (collectAsMap at /home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/mainspark.py:257) finished in 1.032 s
20/12/01 12:32:02 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/01 12:32:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/12/01 12:32:02 INFO DAGScheduler: Job 1 finished: collectAsMap at /home/sreyans/Desktop/SEM5/Big_Data_SEM5/PROJECT_FPL_ANALYTICS/mainspark.py:257, took 1.058135 s
20/12/01 12:32:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 312.0 B, free 412.9 MiB)
20/12/01 12:32:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 412.9 MiB)
20/12/01 12:32:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.2.15:43733 (size: 14.0 KiB, free: 413.8 MiB)
20/12/01 12:32:02 INFO SparkContext: Created broadcast 5 from broadcast at NativeMethodAccessorImpl.java:0
20/12/01 12:32:04 INFO FileBasedWriteAheadLog_ReceivedBlockTracker: Recovered 201 write ahead log files from file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/receivedBlockMetadata
20/12/01 12:32:04 INFO ReceiverTracker: Starting 1 receivers
20/12/01 12:32:04 INFO ReceiverTracker: ReceiverTracker started
20/12/01 12:32:04 INFO PythonStateDStream: Checkpoint interval automatically set to 16000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Duration for remembering RDDs set to 32000 ms for org.apache.spark.streaming.api.python.PythonTransformedDStream@4332ed3b
20/12/01 12:32:04 INFO PythonTransformed2DStream: Duration for remembering RDDs set to 32000 ms for org.apache.spark.streaming.api.python.PythonTransformed2DStream@20f6bf3b
20/12/01 12:32:04 INFO PythonTransformedDStream: Duration for remembering RDDs set to 32000 ms for org.apache.spark.streaming.api.python.PythonTransformedDStream@6d249f89
20/12/01 12:32:04 INFO SocketInputDStream: Duration for remembering RDDs set to 32000 ms for org.apache.spark.streaming.dstream.SocketInputDStream@714f4374
20/12/01 12:32:04 INFO PythonTransformedDStream: Duration for remembering RDDs set to 32000 ms for org.apache.spark.streaming.api.python.PythonTransformedDStream@4c2e4f4e
20/12/01 12:32:04 INFO SocketInputDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO SocketInputDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO SocketInputDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@714f4374
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@6d249f89
20/12/01 12:32:04 INFO SocketInputDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO SocketInputDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO SocketInputDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@714f4374
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@4c2e4f4e
20/12/01 12:32:04 INFO PythonTransformed2DStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformed2DStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformed2DStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformed2DStream@20f6bf3b
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@4332ed3b
20/12/01 12:32:04 INFO SocketInputDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO SocketInputDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO SocketInputDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@714f4374
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@6d249f89
20/12/01 12:32:04 INFO SocketInputDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO SocketInputDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO SocketInputDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@714f4374
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@4c2e4f4e
20/12/01 12:32:04 INFO PythonTransformed2DStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformed2DStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformed2DStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformed2DStream@20f6bf3b
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@4332ed3b
20/12/01 12:32:04 INFO PythonStateDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonStateDStream: Storage level = Memory Deserialized 1x Replicated
20/12/01 12:32:04 INFO PythonStateDStream: Checkpoint interval = 16000 ms
20/12/01 12:32:04 INFO PythonStateDStream: Remember interval = 32000 ms
20/12/01 12:32:04 INFO PythonStateDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonStateDStream@78685314
20/12/01 12:32:04 INFO PythonTransformed2DStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformed2DStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformed2DStream: Remember interval = 8000 ms
20/12/01 12:32:04 INFO PythonTransformed2DStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformed2DStream@4e81844f
20/12/01 12:32:04 INFO PythonTransformedDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO PythonTransformedDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO PythonTransformedDStream: Remember interval = 8000 ms
20/12/01 12:32:04 INFO PythonTransformedDStream: Initialized and validated org.apache.spark.streaming.api.python.PythonTransformedDStream@f008827
20/12/01 12:32:04 INFO ForEachDStream: Slide time = 8000 ms
20/12/01 12:32:04 INFO ForEachDStream: Storage level = Serialized 1x Replicated
20/12/01 12:32:04 INFO ForEachDStream: Checkpoint interval = null
20/12/01 12:32:04 INFO ForEachDStream: Remember interval = 8000 ms
20/12/01 12:32:04 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2bac3b94
20/12/01 12:32:04 INFO DAGScheduler: Got job 2 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
20/12/01 12:32:04 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
20/12/01 12:32:04 INFO DAGScheduler: Parents of final stage: List()
20/12/01 12:32:04 INFO DAGScheduler: Missing parents: List()
20/12/01 12:32:04 INFO DAGScheduler: Submitting ResultStage 2 (Receiver 0 ParallelCollectionRDD[16] at makeRDD at ReceiverTracker.scala:609), which has no missing parents
20/12/01 12:32:04 INFO ReceiverTracker: Receiver 0 started
20/12/01 12:32:04 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 81.6 KiB, free 412.8 MiB)
20/12/01 12:32:04 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 28.6 KiB, free 412.8 MiB)
20/12/01 12:32:04 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.2.15:43733 (size: 28.6 KiB, free: 413.8 MiB)
20/12/01 12:32:04 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1223
20/12/01 12:32:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (Receiver 0 ParallelCollectionRDD[16] at makeRDD at ReceiverTracker.scala:609) (first 15 tasks are for partitions Vector(0))
20/12/01 12:32:04 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/12/01 12:32:04 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 8350 bytes)
20/12/01 12:32:04 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
20/12/01 12:32:04 INFO RecurringTimer: Started timer for JobGenerator at time 1606806128000
20/12/01 12:32:04 INFO JobGenerator: Started JobGenerator at 1606806128000 ms
20/12/01 12:32:04 INFO JobScheduler: Started JobScheduler
20/12/01 12:32:04 INFO StreamingContext: StreamingContext started
20/12/01 12:32:04 INFO RecurringTimer: Started timer for BlockGenerator at time 1606806125000
20/12/01 12:32:04 INFO BlockGenerator: Started BlockGenerator
20/12/01 12:32:04 INFO BlockGenerator: Started block pushing thread
20/12/01 12:32:04 INFO ReceiverTracker: Registered receiver for stream 0 from 10.0.2.15:46741
20/12/01 12:32:04 INFO ReceiverSupervisorImpl: Starting receiver 0
20/12/01 12:32:04 INFO SocketReceiver: Connecting to localhost:6100
20/12/01 12:32:04 INFO SocketReceiver: Connected to localhost:6100
20/12/01 12:32:04 INFO ReceiverSupervisorImpl: Called receiver 0 onStart
20/12/01 12:32:04 INFO ReceiverSupervisorImpl: Waiting for receiver to be stopped
20/12/01 12:32:05 INFO MemoryStore: Block input-0-1606806125000 stored as values in memory (estimated size 515.4 KiB, free 412.3 MiB)
20/12/01 12:32:05 INFO BlockManagerInfo: Added input-0-1606806125000 in memory on 10.0.2.15:43733 (size: 515.4 KiB, free: 413.3 MiB)
20/12/01 12:32:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
20/12/01 12:32:05 WARN BlockManager: Block input-0-1606806125000 replicated to only 0 peer(s) instead of 1 peers
20/12/01 12:32:05 INFO BlockGenerator: Pushed block input-0-1606806125000
20/12/01 12:32:08 INFO PythonStateDStream: Time 1606806120000 ms is invalid as zeroTime is 1606806120000 ms , slideDuration is 8000 ms and difference is 0 ms
20/12/01 12:32:09 INFO JobScheduler: Added jobs for time 1606806128000 ms
20/12/01 12:32:09 INFO JobGenerator: Checkpointing graph for time 1606806128000 ms
20/12/01 12:32:09 INFO DStreamGraph: Updating checkpoint data for time 1606806128000 ms
20/12/01 12:32:09 INFO JobScheduler: Starting job streaming job 1606806128000 ms.0 from job set of time 1606806128000 ms
20/12/01 12:32:09 INFO DStreamGraph: Updated checkpoint data for time 1606806128000 ms
20/12/01 12:32:09 INFO CheckpointWriter: Submitted checkpoint of time 1606806128000 ms to writer queue
20/12/01 12:32:09 INFO CheckpointWriter: Saving checkpoint for time 1606806128000 ms to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806128000'
20/12/01 12:32:09 INFO SparkContext: Starting job: runJob at PythonRDD.scala:154
20/12/01 12:32:09 INFO DAGScheduler: Registering RDD 22 (call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) as input to shuffle 3
20/12/01 12:32:09 INFO DAGScheduler: Registering RDD 30 (call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) as input to shuffle 2
20/12/01 12:32:09 INFO DAGScheduler: Registering RDD 36 (call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) as input to shuffle 1
20/12/01 12:32:09 INFO DAGScheduler: Registering RDD 44 (call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) as input to shuffle 0
20/12/01 12:32:09 INFO DAGScheduler: Got job 3 (runJob at PythonRDD.scala:154) with 1 output partitions
20/12/01 12:32:09 INFO DAGScheduler: Final stage: ResultStage 7 (runJob at PythonRDD.scala:154)
20/12/01 12:32:09 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
20/12/01 12:32:09 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 6)
20/12/01 12:32:09 INFO DAGScheduler: Submitting ShuffleMapStage 3 (PairwiseRDD[22] at call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442), which has no missing parents
20/12/01 12:32:09 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 11.1 KiB, free 412.3 MiB)
20/12/01 12:32:09 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 412.3 MiB)
20/12/01 12:32:09 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.2.15:43733 (size: 6.9 KiB, free: 413.3 MiB)
20/12/01 12:32:09 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1223
20/12/01 12:32:09 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (PairwiseRDD[22] at call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) (first 15 tasks are for partitions Vector(0))
20/12/01 12:32:09 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/12/01 12:32:09 INFO CheckpointWriter: Deleting file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606802552000
20/12/01 12:32:09 INFO CheckpointWriter: Checkpoint for time 1606806128000 ms saved to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806128000', took 10231 bytes and 522 ms
20/12/01 12:32:13 INFO MemoryStore: Block input-0-1606806133000 stored as values in memory (estimated size 464.6 KiB, free 411.8 MiB)
20/12/01 12:32:13 INFO BlockManagerInfo: Added input-0-1606806133000 in memory on 10.0.2.15:43733 (size: 464.6 KiB, free: 412.8 MiB)
20/12/01 12:32:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
20/12/01 12:32:13 WARN BlockManager: Block input-0-1606806133000 replicated to only 0 peer(s) instead of 1 peers
20/12/01 12:32:13 INFO BlockGenerator: Pushed block input-0-1606806133000
20/12/01 12:32:16 INFO PythonStateDStream: Marking RDD 78 for time 1606806136000 ms for checkpointing
20/12/01 12:32:16 INFO JobScheduler: Added jobs for time 1606806136000 ms
20/12/01 12:32:16 INFO JobGenerator: Checkpointing graph for time 1606806136000 ms
20/12/01 12:32:16 INFO DStreamGraph: Updating checkpoint data for time 1606806136000 ms
20/12/01 12:32:16 INFO DStreamGraph: Updated checkpoint data for time 1606806136000 ms
20/12/01 12:32:16 INFO CheckpointWriter: Submitted checkpoint of time 1606806136000 ms to writer queue
20/12/01 12:32:16 INFO CheckpointWriter: Saving checkpoint for time 1606806136000 ms to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806136000'
20/12/01 12:32:17 INFO CheckpointWriter: Deleting file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606802560000
20/12/01 12:32:17 INFO CheckpointWriter: Checkpoint for time 1606806136000 ms saved to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806136000', took 10241 bytes and 110 ms
20/12/01 12:32:21 INFO MemoryStore: Block input-0-1606806141200 stored as values in memory (estimated size 454.1 KiB, free 411.4 MiB)
20/12/01 12:32:21 INFO BlockManagerInfo: Added input-0-1606806141200 in memory on 10.0.2.15:43733 (size: 454.1 KiB, free: 412.4 MiB)
20/12/01 12:32:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.
20/12/01 12:32:21 WARN BlockManager: Block input-0-1606806141200 replicated to only 0 peer(s) instead of 1 peers
20/12/01 12:32:21 INFO BlockGenerator: Pushed block input-0-1606806141200
20/12/01 12:32:24 INFO JobScheduler: Added jobs for time 1606806144000 ms
20/12/01 12:32:24 INFO JobGenerator: Checkpointing graph for time 1606806144000 ms
20/12/01 12:32:24 INFO DStreamGraph: Updating checkpoint data for time 1606806144000 ms
20/12/01 12:32:24 INFO DStreamGraph: Updated checkpoint data for time 1606806144000 ms
20/12/01 12:32:24 INFO CheckpointWriter: Submitted checkpoint of time 1606806144000 ms to writer queue
20/12/01 12:32:24 INFO CheckpointWriter: Saving checkpoint for time 1606806144000 ms to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806144000'
20/12/01 12:32:24 INFO CheckpointWriter: Deleting file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606802568000
20/12/01 12:32:24 INFO CheckpointWriter: Checkpoint for time 1606806144000 ms saved to file 'file:/usr/local/spark-3.0.1-bin-hadoop3.2/checkpoint_FPL/checkpoint-1606806144000', took 10257 bytes and 74 ms
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Received stop signal
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Stopping receiver with message: Stopped by driver: 
20/12/01 12:32:24 INFO ReceiverTracker: Sent stop signal to all 1 receivers
20/12/01 12:32:24 WARN SocketReceiver: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)
20/12/01 12:32:24 INFO SocketReceiver: Closed socket to localhost:6100
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Called receiver onStop
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Deregistering receiver 0
20/12/01 12:32:24 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Stopped receiver 0
20/12/01 12:32:24 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data
java.net.SocketException: Socket closed
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)
	at java.net.SocketInputStream.read(SocketInputStream.java:171)
	at java.net.SocketInputStream.read(SocketInputStream.java:141)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:161)
	at java.io.BufferedReader.readLine(BufferedReader.java:324)
	at java.io.BufferedReader.readLine(BufferedReader.java:389)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)
	at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
	at org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)
	at org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)
20/12/01 12:32:24 INFO ReceiverSupervisorImpl: Stopping receiver with message: Restarting receiver with delay 2000ms: Error receiving data: java.net.SocketException: Socket closed
20/12/01 12:32:24 WARN ReceiverSupervisorImpl: Receiver has been stopped
20/12/01 12:32:24 INFO BlockGenerator: Stopping BlockGenerator
20/12/01 12:32:25 INFO RecurringTimer: Stopped timer for BlockGenerator after time 1606806145000
20/12/01 12:32:25 INFO BlockGenerator: Waiting for block pushing thread to terminate
20/12/01 12:32:25 INFO BlockGenerator: Pushing out the last 0 blocks
20/12/01 12:32:25 INFO BlockGenerator: Stopped block pushing thread
20/12/01 12:32:25 INFO BlockGenerator: Stopped BlockGenerator
Exception in thread "receiver-supervisor-future-0" java.lang.Error: java.lang.InterruptedException: sleep interrupted
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
20/12/01 12:32:25 INFO ReceiverSupervisorImpl: Stopped receiver without error
20/12/01 12:32:25 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 923 bytes result sent to driver
20/12/01 12:32:25 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, 10.0.2.15, executor driver, partition 0, PROCESS_LOCAL, 7255 bytes)
20/12/01 12:32:25 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 20477 ms on 10.0.2.15 (executor driver) (1/1)
20/12/01 12:32:25 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) finished in 20.656 s
20/12/01 12:32:25 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/12/01 12:32:25 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
20/12/01 12:32:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
20/12/01 12:32:25 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/12/01 12:32:25 INFO ReceiverTracker: All of the receivers have deregistered successfully
20/12/01 12:32:25 INFO BatchedWriteAheadLog: BatchedWriteAheadLog shutting down at time: 1606806145084.
20/12/01 12:32:25 WARN BatchedWriteAheadLog: BatchedWriteAheadLog Writer queue interrupted.
20/12/01 12:32:25 INFO BatchedWriteAheadLog: BatchedWriteAheadLog Writer thread exiting.
20/12/01 12:32:25 INFO FileBasedWriteAheadLog_ReceivedBlockTracker: Stopped write ahead log manager
20/12/01 12:32:25 INFO ReceiverTracker: ReceiverTracker stopped
20/12/01 12:32:25 INFO JobGenerator: Stopping JobGenerator immediately
20/12/01 12:32:25 INFO RecurringTimer: Stopped timer for JobGenerator after time 1606806144000
20/12/01 12:32:25 INFO CheckpointWriter: CheckpointWriter executor terminated? true, waited for 5 ms.
20/12/01 12:32:25 INFO JobGenerator: Stopped JobGenerator
20/12/01 12:32:25 INFO BlockManager: Found block input-0-1606806125000 locally
20/12/01 12:32:27 INFO JobScheduler: Stopped JobScheduler
20/12/01 12:32:27 INFO StreamingContext: StreamingContext stopped successfully
20/12/01 12:32:27 INFO SparkUI: Stopped Spark web UI at http://10.0.2.15:4040
20/12/01 12:32:27 INFO DAGScheduler: Job 3 failed: runJob at PythonRDD.scala:154, took 18.139470 s
20/12/01 12:32:27 INFO DAGScheduler: ShuffleMapStage 3 (call at /usr/local/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:2442) failed in 17.893 s due to Stage cancelled because SparkContext was shut down
20/12/01 12:32:27 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/12/01 12:32:27 INFO MemoryStore: MemoryStore cleared
20/12/01 12:32:27 INFO BlockManager: BlockManager stopped
20/12/01 12:32:27 INFO BlockManagerMaster: BlockManagerMaster stopped
20/12/01 12:32:27 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/12/01 12:32:27 INFO SparkContext: Successfully stopped SparkContext
20/12/01 12:32:27 INFO SparkContext: SparkContext already stopped.
20/12/01 12:32:27 INFO PythonRunner: Times: total = 1230, boot = 1002, init = 97, finish = 131
20/12/01 12:32:27 ERROR TaskContextImpl: Error in TaskCompletionListener
java.lang.IllegalStateException: Block broadcast_7 not found
	at org.apache.spark.storage.BlockInfoManager.$anonfun$unlock$3(BlockInfoManager.scala:293)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.storage.BlockInfoManager.unlock(BlockInfoManager.scala:293)
	at org.apache.spark.storage.BlockManager.releaseLock(BlockManager.scala:1145)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1(TorrentBroadcast.scala:279)
	at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$releaseBlockManagerLock$1$adapted(TorrentBroadcast.scala:279)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:125)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$markTaskCompleted$1$adapted(TaskContextImpl.scala:124)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1(TaskContextImpl.scala:137)
	at org.apache.spark.TaskContextImpl.$anonfun$invokeListeners$1$adapted(TaskContextImpl.scala:135)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:135)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:124)
	at org.apache.spark.scheduler.Task.run(Task.scala:137)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/12/01 12:32:27 ERROR Utils: Uncaught exception in thread Executor task launch worker for task 3
java.lang.NullPointerException
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:148)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.scheduler.Task.run(Task.scala:146)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
20/12/01 12:32:27 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3): Block broadcast_7 not found

Previous exception in task: /tmp/blockmgr-5e063467-4451-4e79-b51f-5123116347fb/0a/shuffle_3_3_0.index.5b29bd26-cb63-443c-8ee7-ae6ae9bfa071 (No such file or directory)
	java.io.FileOutputStream.open0(Native Method)
	java.io.FileOutputStream.open(FileOutputStream.java:270)
	java.io.FileOutputStream.<init>(FileOutputStream.java:213)
	java.io.FileOutputStream.<init>(FileOutputStream.java:162)
	org.apache.spark.shuffle.IndexShuffleBlockResolver.writeIndexFileAndCommit(IndexShuffleBlockResolver.scala:184)
	org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter.commitAllPartitions(LocalDiskShuffleMapOutputWriter.java:117)
	org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.writePartitionedData(BypassMergeSortShuffleWriter.java:222)
	org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:167)
	org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	org.apache.spark.scheduler.Task.run(Task.scala:127)
	org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
	org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
	org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
	java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	java.lang.Thread.run(Thread.java:748)
20/12/01 12:32:28 INFO ShutdownHookManager: Shutdown hook called
20/12/01 12:32:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-f34ff39b-56cb-4548-b178-65ee4e2158d7
20/12/01 12:32:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9521def5-5b18-461f-90d4-888a65e55038
20/12/01 12:32:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-9521def5-5b18-461f-90d4-888a65e55038/pyspark-4389363d-8899-4163-8677-36a35c7e6dd0
